<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[2017年腾讯QQ红包]]></title>
    <url>%2F2018%2F03%2F03%2F2017%E5%B9%B4%E8%85%BE%E8%AE%AFQQ%E7%BA%A2%E5%8C%85%2F</url>
    <content type="text"><![CDATA[2017年腾讯QQ红包 系统架构和数据流以上系统最大优点就应用层操作系统完全和db隔离，数据持久化的步骤：data-&gt;redis-&gt;rabbitmq-&gt;mysql，但是还存在db单点问题，依赖无关系统Kafka影响系统性能。 系统优化过程根据压测报告和日志分析，不断优化性能短板。 应用内1.资源锁的问题2.多线程并发3.去掉重复冗余的代码 tomcat连接池1.加大连接数2.加大排队数(并不会加大TPS) http1.TCP并发链接(没有及时释放)，CLOSE_WAIT过多。2.加大连接池连接数量 mysql1.优化sql，避免复杂查询2.优化索引,让每条select都走索引3.加大连接池的最大连接数4.尝试测试不同的连接池,选择性能最佳的 redis1.db中的数据load保存为hash（全表保存）2.根据业务优化redis存储结构,减少redis查询次数3.redis cpu为单核，大量查询会成为严重短板,进行分片处理和集群处理4.过期时间不设置,可以测试出用户消耗的最大redis容量 加快消费者消费速度1.增加消费者数量2.增加消费者预读取数据数量3.优化消费逻辑，例如db查询，逻辑判断 mq1.消息体一般为redis key,可以去redis拿取数据,优化消息存储大小2.可以按功能不同,拆分多个队列,加快单逻辑处理速度 java启动脚本优化1.spring boot项目2.优化jvm启动参数 项目初始化1.将数据库数据先load到redis。可以控制速度，百万数据大概5min load完。 日志优化1.log4j2异步写日志文件2.减少日志打印，例如正常请求仅打印入参和出参3.搭建日志系统，自动收集服务器的日志，然后通过kafka发送给elk 作者 [soar]2018年 03月 03日]]></content>
      <tags>
        <tag>架构设计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[客户端重复请求]]></title>
    <url>%2F2017%2F10%2F11%2Fclient-request-double%2F</url>
    <content type="text"><![CDATA[客户端请求重复导致流量翻倍 发现问题今天在对向项目进行接口测试时，通过fiddler抓包，发现有同一个接口在同一时间请求了两次，第一次请求的url末尾一个没带/，第二次请求带了/，如下所示： 导致的问题因为重复请求了，所以会导致这样会导致流量翻倍。 分析问题首先分析客户端发出的请求的过程，c-&gt;dns-&gt;elb-&gt;nginx-&gt;tomcat，通过nginx和tomcat记录的日志发现第一次请求没走到tomcat，在nginx这层就被重定向了，导致301，所以初步确定是nginx配置路劲的问题。查看nginx配置发现，如下所示：发现该nginx转发配置了要请求的URL带上/，所以初步判断是由于第一次没有带/，进行了重定向301，然后nginx对该请求自动带上了/。 解决问题只要去掉location中的最后的/，然后重启nginx即可解决问题。 作者 [soar]2017年 10月 11日]]></content>
      <tags>
        <tag>线上事故</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[后台接口参数不兼容]]></title>
    <url>%2F2017%2F10%2F03%2F%E5%90%8E%E5%8F%B0%E6%8E%A5%E5%8F%A3%E5%8F%82%E6%95%B0%E4%B8%8D%E5%85%BC%E5%AE%B9%2F</url>
    <content type="text"><![CDATA[后台接口参数不兼容导致的线上事故 前言刚进公司实习不久，第一次遇到线上事故，在解决事故中积极的配合着老大的指挥，事后收获颇大。 系统架构在这里首先描述下我们当时出现问题所在的架构是怎样的，因为对后续的解决问题非常有帮助，具体如下图所示：其次我们整个个服务共有7子系统，分别是运营系统，商户系统，用户系统，统计系统（主要是客户端日志上报），状态系统（客户端心跳检测），异步消息消息以及服务器报警系统，其中用户系统，统计系统，状态系统公用一个域名。 发现问题2017年10月03日的15点40分，客户反馈客户端的奖励列表和排行榜获取数异常，也就是很慢。 导致的问题用户侧的整个系统的所有接口无返回或者返回很慢。 分析问题&ensp;&ensp;&ensp;&ensp;因为问题导致了整个用户侧系统的接口返回超慢，最先怀疑是不是带宽被占满了。所以第一时间登录腾讯云查看我们用户侧所依赖的8台nginx的带宽使用情况，发现所有的nginx的服务器的带宽全部占满。&ensp;&ensp;&ensp;&ensp;出现这种问题肯定会怀疑是不是被黑了或者因为客户端某个接口过度频繁调用服务端的接口导致，因为在这里我们更怀疑是客户端的某个接口过度调用导致了，所以马上通知客户端的小伙伴看看新版本做了哪些迭代，是不是有什么问题（因为2个小时之前我们的个客户端在2千多家网吧发布了），同时后端也采取相应的解决措施，当时就决定如果5-10分钟之内确定不了问题，我们就采取版本回退的方法。&ensp;&ensp;&ensp;&ensp;此时后台开发人员立马登录其中一台nginx服务器查看日志，发现tail不了日志，机器的CPU和内存，以及IO都被占满了，LB摘除这台nginx服务器，登录tail最近一万条日志一分钟发现日志上报系统的接口达到了35000条，接口的QPS达到了4800,到此时也就发现了问题，是由于客户端频繁的调用了统计系统的日志上报接口导致的，此时客户端的开发也确定了在日志上报接口调用异常时会不断重试，止于是什么原因导致接口调用异常稍后确定。 解决问题客户端取消日志上报接口，并且在状态系统控制了客户端版本不是最新的就全部关闭，最后全量发布。因为用户系统，统计系统，状态系统公用一个域名，共享同样的带宽，带宽基本被统计系统占满，状态系统的心跳基本无法触及客户端，客户端关闭速度很缓慢。此时首先提升其中一台nginx的带宽为40M，然后提高状态系统的tomcat线程数为5000（原值为500），最后清空状态系统所在服务器的磁盘（之前请求暴增导致日志体积过大，磁盘占有率过高），以此来保证有足够的带宽提供给状态系统处理心跳接口-&gt;心跳接口控制客户端不是最新版本就关闭软件，到了差不多17点时，系统基本稳定了。 事故原因后台15:30左右状态系统在日志上报接口中增加一个参数path，但是没有标注（require=false）；客户端由于参数缺失导致接口400。 反思与总结设计不合理这次事故的主要原因还是另一个后台同事临时在状态系统的接口里增加请求参数，但是没有考虑到兼容性的问题，同时也暴露出了客户端重试机制不严谨，以及架构设计方面的多个项目使用同一个域名的隐患。 分析问题方向有误因为问题是突然出现，所以在出现问题的时候就应该考虑下客户端和后台在之前做了什么改变，就算想不到，在nginx服务器观察日志也应该看得到接口400是因为接口的参数的问题，实际上这两点都没有分析到。如果之前能马上想到正确的事故原因，那解决问题就很简单，只要后台发版即可快速的解决问题。及时是客户端的问题，既可以采用上面的解决为的方案，但是在保证有足够的带宽提供给状态系统处理心跳接口-&gt;心跳接口控制客户端不是最新版本就关闭软件时，其实不需要提升nginx的带宽，有一种高效快速的方法就是在nginx就直接对日志上报接口返回200即可，这样大大减少了请求时间，具体如下图所示： 如何快速定位和解决IDC问题作为web后台开发人员，但实际上用户发起一个http请求，要经过很多中间步骤才到你的服务器（例如浏览器缓存、DNS、LB、nginx，tomcat等），服务器一般又会经过很多处理才到你写的那部分代码（权限，路由等），这整个流程中的很多系统或者步骤，绝大部分人是不可能去参与写代码的，但掌握了这些知识对你的综合水平有很大作用，例如方案设计、线上故障处理这些更加有含金量的技术工作都需要综合技术水平。 作者 [soar]2017年 11月 03日]]></content>
      <tags>
        <tag>线上事故</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何合理的设计接口]]></title>
    <url>%2F2017%2F09%2F01%2F%E5%A6%82%E4%BD%95%E5%90%88%E7%90%86%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%8E%A5%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[如何合理的设计接口 背景这里主要以信贷上传用户实心信息为例，在编码业务代码时，必须是基于设计思想，考虑可能问题，然后编码不同场景下不同用户操作的测试用例，最后进行断点调试，单元测试，接口测试，以及自动化测试。 设计思想可扩展，复用，解耦，简易性，必要性等。 考虑问题安全性，用户体验，性能等。 业务场景1.正常拍照2.非正常拍照3.通过接口传数据，同2 用户操作1.用户使用自己的身份证(是否在库)2.用户使用别人的身份证(是否在库)3.使用错误的身份证 业务代码1.校验身份证：数据合法性，身份证有效日期，判断该卡是否被使用(二要素鉴权，其实是没必要的);2.提交实名信息：判断身份证是否被使用，人脸二要素识别，实名信息落库。 作者 [soar]2017年 09月 01日]]></content>
      <tags>
        <tag>开发之道</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka高性能之道]]></title>
    <url>%2F2017%2F06%2F15%2Fkafka%E9%AB%98%E6%80%A7%E8%83%BD%E4%B9%8B%E9%81%93%2F</url>
    <content type="text"><![CDATA[kafka高性能之道 背景想要了解kafka的高性能的知识点，首先必须之道kafka整个处理消息的流程，这里主要以客户端发送消息-&gt;broker-&gt;消息者消费消息来描述。 生产者发送消息并行发送消息Partition是Kafka中横向扩展和一切并行化的基础，每个Topic都至少被切分为1个Partition，至于配置多少个Partition，如下分析：服务器配置如下：CPU：8 vCPU， Intel(R) Xeon(R) CPU E5-2680 v2 @ 2.80GHz，2 Sockets，4 Cores per socket，1 Thread per core内存：16 GB磁盘：500 GB实验条件：3个Broker，1个Topic，无Replication，异步模式，3个Producer，消息Payload为100字节测试项目：分别测试1到9个Partition时的吞吐量测试结果：不同Partition数量时的集群总吞吐率如下图所示： 批处理批处理是一种常用的用于提高I/O性能的方式。对Kafka而言，批处理既减少了网络IO传输的次数，又提高了写磁盘的效率。 数据压缩降低网络负载Kafka从0.7开始，即支持将数据压缩后再传输给Broker。除了可以将每条消息单独压缩然后传输外，Kafka还支持在批量发送时，将整个Batch的消息一起压缩后传输。数据压缩的一个基本原理是，重复数据越多压缩效果越好。因此将整个Batch的数据一起压缩能更大幅度减小数据量，从而更大程度提高网络传输效率。Broker接收消息后，并不直接解压缩，而是直接将消息以压缩后的形式持久化到磁盘。Consumer Fetch到数据后再解压缩。因此Kafka的压缩不仅减少了Producer到Broker的网络传输负载，同时也降低了Broker磁盘操作的负载，也降低了Consumer与Broker间的网络传输量，从而极大得提高了传输效率，提高了吞吐量。 高效的序列化方式Kafka消息的Key和Payload（或者说Value）的类型可自定义，只需同时提供相应的序列化器和反序列化器即可。因此用户可以通过使用快速且紧凑的序列化-反序列化方式（如Avro，Protocal Buffer）来减少实际网络传输和磁盘存储的数据规模，从而提高吞吐率。这里要注意，如果使用的序列化方法太慢，即使压缩比非常高，最终的效率也不一定高。 brokerPageCacheKafka重度依赖底层操作系统提供的PageCache功能。当上层有写操作时，操作系统只是将数据写入PageCache，同时标记Page属性为Dirty。当读操作发生时，先从PageCache中查找，如果发生缺页才进行磁盘调度，最终返回需要的数据。实际上PageCache是把尽可能多的空闲内存都当做了磁盘缓存来使用。同时如果有其他进程申请内存，回收PageCache的代价又很小，所以现代的OS都支持PageCache。PageCache还只是第一步，Kafka为了进一步的优化性能还采用了Sendfile技术，也就是接下来说明的Zero Copy。 PageCache优点所有的In-Process Cache在OS中都有一份同样的PageCache。所以通过将缓存只放在PageCache，可以至少让可用缓存空间翻倍。 JVM GC缺点使用PageCache功能同时可以避免在JVM内部缓存数据，如果在Heap内管理缓存，JVM的GC线程会频繁扫描Heap空间，带来不必要的开销。如果Heap过大，执行一次Full GC对系统的可用性来说将是极大的挑战。 消费者消费下消息在这里消费者有两种消费消息的模式，一种是轮训poll，另一种就是订阅。无论是哪种，broker都要进行数据的准备。 PageCache详见broker-&gt;PageCache描述 从磁盘读取数据方式（Sequence I/O）Sequence I/O: 600MB/sRandom I/O: 100KB/s Zero Copy详解网址：http://ysjun.github.io/2016/05/14/zero-copy/，优点：sendfile系统调用实现零拷贝，减少上下文切换。 作者 [soar]2017年 06月 15日]]></content>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql字段带有回车换行符号]]></title>
    <url>%2F2016%2F09%2F15%2Fmysql-field-newline%2F</url>
    <content type="text"><![CDATA[mysql字段带有回车换行符号引发数据为空的问题 发现问题在一次实践中发现，明明看到数据库表中某个字段有数据，但是通过程序去查，没有查出来。 分析问题出现这个问题之后，然后去点击这个字段的数据，发现数据为空，最后才发现了问题所在，因为这个数据里面携带回车换行符号岛导致获取的数据为空。 解决问题删除该字段的回车换行符即可解决这个问题。 作者 [soar]2016年 09月 15日]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java工具类]]></title>
    <url>%2F2016%2F06%2F20%2Fjava-utils%2F</url>
    <content type="text"><![CDATA[GitHub上前16名的Java工具类 在Java中，工具类就是一个定义了一组执行常用功能方法的类。programcreek统计了最常用的Java工具类及其常用方法。数据来自GitHub随机选择的50,000个开源Java项目。 下面是前16位的最受欢迎的工具类和方法，收藏一下以便在需要的时候不需要自己重复发明轮子。 方法的名称通常就表示其功能。 作者 [soar]2016年 06月 20日]]></content>
      <tags>
        <tag>java utils</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[web高并发案例分析]]></title>
    <url>%2F2016%2F06%2F09%2Fcurrent-case%2F</url>
    <content type="text"><![CDATA[一般来说web高并发需要解决的问题如下： 1.应用服务器的高并发 2.数据库的高并发 持久化的数据mysql或者oracle（关系型） 缓存型数据库redis或者memcache（非关系型） 一、应用服务器的高并发一般应用服务器的架构都是对台tomcat，加上一台nginx反向代理服务器组成一个小型集群，这样虽然可以承受的住一般的高并发访问，但是对于高并发的话，就有些吃紧。可能大家会觉得tomcat可以不断水平扩容，这样就可以承受的住高并发访问，但是要知道一点，一台nginx服务不管怎么去优化（提升服务器的硬件配置），她的并发数终究是有限的，当达到十万级时，一台代理服务器时扛不住这么大的并发数的，因为需要多台nginx组成一个集群，自己可以搭建个均衡负载器，但是工作量很大，对于没有这方面经验来说，可以使用腾讯云的均衡负载器。每次需要扩容时，可以看看系统还能不能继续优化，比如看看：tomcat和jvm参数设置是不是最优等。 二、对于数据库的并发，一般包括持久化数据库和缓存数据的并发1. 对于持久化的数据库，对于sql，能够适当增加索引，sql技巧等来优化数据库，如果不行，再采用垂直和水平分表，垂直和水平分库，分区以及读写分离等操作。1.1 sql技巧：一般业务下都会有按时间降序的要求，此时可以根据主键id进行降序排序，一样能达到业务要去，但是这种方法要快速很多。 1.2 垂直分表：把不常用或业务逻辑不紧密或存储内容比较多的字段分到新的表中可使表存储更多数据。另外垂直分割可以使得数据行变小，一个数据页就能存放更多的数据，在查询时就会减少I/O次数和增加缓存的命中率。其缺点是需要管理冗余列，查询所有数据需要join操作。 1.3 水平分表：一般来说，水平分表更多的是采用某种策略，比如存放游戏日志，这里先不讨论用mongodb或者habse来存储，我们可以按天，星期或者月来分表，至于采用哪种根据表的产生数据的速度，另外，我们再根据日期进行分表的同时，还可以根据用户的唯一性标识进行分表，比如：用身份标识（qq）,我们可以根据qq的后两位根据分表，这样最多是100张表，为了限制单张表的数据无限增长，可以做个任务定时器去定时清楚表的数据，最好保持在10万以下；冷热数据存储也是水平分表的另一种方法。 1.4 垂直分库：垂直分库在“微服务”盛行的今天已经非常普及了。基本的思路就是按照业务模块来划分出不同的数据库，而不是像早期一样将所有的数据表都放到同一个数据库中。 比如：对于游戏日志放到一个数据库，在这里就要考虑到在高并发场景下，垂直分库一定程度上能够突破IO、连接数及单机硬件资源的瓶颈，是大型分布式系统中优化数据库架构的重要手段。但是也会遇到很多问题，例如：跨库join，分布式事务等。 1.5 水平分库: 与上面讲到的水平分表的思想相同，唯一不同的就是将这些拆分出来的表保存在不同的数据中。这也是很多大型互联网公司所选择的做法。某种意义上来讲，有些系统中使用的”冷热数据分离”（将一些使用较少的历史数据迁移到其他的数据库中。而在业务功能上，通常默认只提供热点数据的查询），也是类似的实践。在高并发和海量数据的场景下，分库能够有效缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源的瓶颈。当然，投入的硬件成本也会更高。同时，这也会带来一些复杂的技术问题和挑战（例如：跨分片的复杂查询，跨分片事务等） 1.6 分区： 1.7 读写分离： 2.缓存数据库redis单点：一般来说系统初期的话，redis都是单点，单点的坏处就是并发数有点，可用性差，可扩性差。在改变架构之前，最好看看原先redis性能能不能优化，一般优化包括配置参数，内存，命中率，对于内存，如果对key能设置过期时间过好设置过期时间并且采用合适的数据淘汰算法，在这里切记对于可能无限增长的key一定注意，比如：time key,每隔10秒设置一个key缓存，这样的话，一年下来就可能有50多万的key,想想就可怕，这样的话可能就要换个数据接口进行存储，或者加上过期时间，再或者换个其他方法，所以在设置key最好想清楚用什么数据接构存储，能不能加上过期时间，最后考虑全局使用合适的数据淘汰算法。 分布式：随着系统的并发数越来越大，可以扩展redis成分布式redis，在这里除了要清楚运维的操作方面：a.增加和移除主从节点 b.重新分片在这里移除主节点需要注意两点：a.先移除主节点的slot b.如果主节点有从节点，会从主节点下的所有的从节点进行选举一个成为主节点，所以一定要确保所有的从节点的本地dump数据文件不能为空；还需要考虑分布式锁和分布式事物。 分布式锁： 分布式事务： 三、小结1.对于应用服务器的高并发： 采用多nginx+多tomcat集群模式，这里还可以根据服务的划分（soa），把复杂的系统划分为一个个子服务，服务之间访问可以采用http+json，也可以采用RPC，一般来说RPC适合内网通信，http+json适合外网通信，当然RPC各个方面的性能都要比http要，其次分布式的节点也可以作为一个集群来部署，这样就能非常高的并发数。 2.数据库的高并发：前面讲了很常见的mysql和redis，在大型网站中数据量是非常大的，对于某些业务每天可能都会生成十万级，百万级，甚至千万级的数据的来书，mysql无疑是够呛的，这时可以考虑使用mongodb或者hbase. 四、案例分析1.文章阅读数增加 一般会用到redis, mysql, 异步消息队列系统（redis：优点：低延迟，缺点：no have ack），具体操作：首先对redis的数据进行加jedisClient.hincrBy(RDU.READ_COUNT, id, 1)，然后返回数据，之后的数据持久化操作就交给redis异步消息系统，这里需要考虑三个fail点：a.redis fail直接返回错误提示 b.请求发送失败 c.异步消息系统处理持久化失败对于第一个我们并不care；第二个请求发送失败（失败点：网络连接失败），这里我们没必要关注这种概率极低的现象，因为你要考虑要技术实现的成本与时间的问题，并且每次发送个异步消息的数据都是最新点赞数，就算出现问题只要再次持久化到数据库就行；第三个异步消息处理失败我们可以做好系统的监控与预警，及时处理失败的业务。 扩展：单点的redis并发数有限，可用性差，可扩性差，可以搭建redis集群来解决这个问题，对于一些可能重要的业务数据需要ack来说，可以采用阿里的racketmq，对于一些不重要的业务数据，丢失也没什么关系，比如：日志处理，可以采用kafka，她无疑是mq性能中的巨无霸。 2.文章搜索 这里不讨论如何实现文章的搜索，文章搜索的实现会在另一篇文章中进行总结分享。一般除了要返回查询的文章的数据，还需要对用户行为的日志进行记录，用于统计和分析用户的行为，方便平台能够能好的迎合用户的需求，这里的日志输入采用了slf4j+logback，日志收集采用了ftp，日志存储采用了mongodb，日志统计与分析可用spring tsak定时任务定统计，展现给分析人员进行分析。这里的除了日志输出在本系统，其他都可以交给日志分析系统，也就是统计系统来做，可以提高单个系统的并发和降低系统的耦合度，日志分析系统主要采用了slf4j+logback+ftp+mongodb,当然还可以采用性能更优的slf4j+logback+kafka+mongodb+Elasticsearch+kibana，其中Elasticsearch是个开源分布式搜索引擎，它的特点有：分布式，零配置，自动发现，索引自动分片，索引副本机制，restful风格接口，多数据源，自动搜索负载等，kibana也是一个开源和免费的工具，他Kibana可以为ElasticSearch 提供的日志分析友好的 Web 界面，可以帮助您汇总、分析和搜索重要数据日志。 五、总结对于高并发的Web系统，一般都会用到集群，分布式，分布式缓存，分布式消息队列，分布式日志收集等技术，这些技术不仅能解决高并发的问题，还可以构建高并发，高流量，高性能，高可用以及可扩展的大型web系统。 作者 [soar]2016年 06月 09日]]></content>
      <tags>
        <tag>current</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何快速的解决线上的问题]]></title>
    <url>%2F2016%2F06%2F08%2Fsolve-question%2F</url>
    <content type="text"><![CDATA[问题：在多线程和高并发环境下，如果有一个平均运行一百万次才出现一次的bug，你如何调试这个bug？ 先问清楚bug是属于哪一类，是崩溃？还是数据不一致？以及对基于这个bug的业务对系统造成的影响严不严重然后尽可能的问更多的资料，才能考虑用什么方法。在考虑用什么方法之前应该从管理上要考虑bug的严重性与成本/时间的问题。 1.选择解决问题的方法，无非就两种： a.解决 b.回退版本+解决，以确保正常业务不出问题 如果bug的严重性大，并且不能马上（1-3分钟）解决问题的话，采用a方法； 如果bug的严重性不大， 采用b方法。 2.无论选择哪种，都要去解决bug，在解决bug之前，尽量保留现场，有利于快速的去解决问题，具体方法如下(都是在测试环境进行):a.重现。一般来说能重现的问题都不是问题，如果事先已经打印了日志，可以根据日志去重现问题， 否则可以根据review code+压力测试去重现问题，如果重现出了问题进行c步骤，否则进行b步骤; b.加大测试力度或者采取别的测试方法去重现问题 c.定位和Reduce(逐步收窄范围)，定位可以采取日志跟踪，断点调试，或者软件调式都可以， 在定位的过程中最逐步的去收窄范围，这样有利于解决快速的定位出问题； d.在定位出了问题之后再次考虑bug的严重性与成本/时间的问题，看需不需要去解决。 3.如果最终能找出问题，需要研究怎样防范相似的 bug。4.总结一般来说，如果是核心业务，都会打出日志，根据日志都能快速解决问题，否则，只能去重现问题，然后去定位问题，最后solve。 作者 [soar]2016年 06月 08日 link &amp; more:http://dbaplus.cn/news-21-625-1.htmlhttps://www.zhihu.com/question/43416744]]></content>
      <tags>
        <tag>运维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux-io]]></title>
    <url>%2F2016%2F06%2F07%2Flinux-io%2F</url>
    <content type="text"><![CDATA[是时候表演真正的技术了-linux IO 本文讨论的背景是Linux环境下的Network IO。本文最重要的参考文献是Richard Stevens的“UNIX® Network Programming Volume 1, Third Edition: The Sockets Networking ”，6.2节“I/O Models”. Stevens在文章中一共比较了五种IO Model： blocking IO nonblocking IO IO multiplexing signal driven IO asynchronous IO 由于signal driven IO在实际中并不常用，所我这只提及剩下的四种IO Model。再说一下IO发生时涉及的对象和步骤。对于一个network IO (这里我们以read举例)，它会涉及到两个系统对象，一个是调用这个IO的process (or thread)，另一个就是系统内核(kernel)。当一个read操作发生时，它会经历两个阶段： 等待数据准备 (Waiting for the data to be ready) 将数据从内核拷贝到进程中 (Copying the data from the kernel to the process) 记住这两点很重要，因为这些IO Model的区别就是在两个阶段上各有不同的情况。 Blocking IO在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程大概是这样： 当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据。对于network io来说，很多时候数据在一开始还没有到达（比如，还没有收到一个完整的UDP包），这个时候kernel就要等待足够的数据到来。而在用户进程这边，整个进程会被阻塞。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。所以，blocking IO的特点就是在IO执行的两个阶段都被block了。 Non-Blocking IOlinux下，可以通过设置socket使其变为non-blocking。当对一个non-blockingsocket执行读操作时，流程是这个样子： 从图中可以看出，当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error.从用户进程角度讲，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的systemcall，那么它马上就将数据拷贝到了用户内存，然后返回。所以，用户进程其实是需要不断的主动询问kernel数据好了没有。 IO MultiplexingIO multiplexing这个词可能有点陌生，但是如果我说select，epoll，大概就都能明白了。有些地方也称这种IO方式为event drivenIO。我们都知道，select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。它的基本原理就是select/epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。它的流程如图： 当用户进程调用了select，那么整个进程会被block，而同时，kernel会“监视”所有select负责的socket，当任何一个socket中的数据准备好了，select就会返回。这个时候用户进程再调用read操作，将数据从kernel拷贝到用户进程。这个图和blockingIO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blockingIO只调用了一个systemcall(recvfrom)。但是，用select的优势在于它可以同时处理多个connection。（多说一句。所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blockin IO的webserver性能更好，可能延迟还更大。select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。） 在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给block。 Asynchronous I/Olinux下的asynchronous IO其实用得很少。先看一下它的流程： 用户进程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从kernel的角度，当它受到一个asynchronous read之后，首先它会立刻返回，所以不会对用户进程产生任何block。然后，kernel会等待数据准备完成，然后将数据拷贝到用户内存，当这一切都完成之后，kernel会给用户进程发送一个signal，告诉它read操作完成了。 到目前为止，已经将四个IO Model都介绍完了。现在回过头来回答最初的那几个问题：blocking和non-blocking的区别在哪，synchronous IO和asynchronous IO的区别在哪。 1. blocking vs non-blocking前面的介绍中其实已经很明确的说明了这两者的区别。调用blocking IO会一直block住对应的进程直到操作完成，而non-blocking IO在kernel还准备数据的情况下会立刻返回。 ###2. synchronous IO vs asynchronous IO 在讲她们之间的区别之前，需要先给出两者的定义。Stevens给出的定义（其实是POSIX的定义）是这样子的： A synchronous I/O operation causes the requesting process to be blocked until that I/O operation completes;An asynchronous I/O operation does not cause the requesting process to be blocked; 两者的区别就在于synchronous IO做”IO operation”的时候会将process阻塞。按照这个定义，之前所述的blocking IO，non-blocking IO，IO multiplexing都属于synchronous IO。可能有人会说，non-blocking IO并没有被block啊。这里有个非常“狡猾”的地方，定义中所指的”IO operation”是指真实的IO操作，就是例子中的recvfrom这个system call。non-blocking IO在执行recvfrom这个system call的时候，如果kernel的数据没有准备好，这时候不会block进程。但是，当kernel中数据准备好的时候，recvfrom会将数据从kernel拷贝到用户内存中，这个时候进程是被block了，在这段时间内，进程是被block的。而asynchronous IO则不一样，当进程发起IO 操作之后，就直接返回再也不理睬了，直到kernel发送一个信号，告诉进程说IO完成。在这整个过程中，进程完全没有被block。 各个IO Model的比较如图所示： 经过上面的介绍，会发现non-blocking IO和asynchronous IO的区别还是很明显的。在non-blocking IO中，虽然进程大部分时间都不会被block，但是它仍然要求进程去主动的check，并且当数据准备完成以后，也需要进程主动的再次调用recvfrom来将数据拷贝到用户内存。而asynchronous IO则完全不同。它就像是用户进程将整个IO操作交给了他人（kernel）完成，然后他人做完后发信号通知。在此期间，用户进程不需要去检查IO操作的状态，也不需要主动的去拷贝数据。 作者 [soar]2016年 06月 07日]]></content>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[zero copy]]></title>
    <url>%2F2016%2F05%2F14%2Fzero-copy%2F</url>
    <content type="text"><![CDATA[Zero Copy分析 传统的I/O使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序: 12File.read(fileDesc, buf, len);Socket.send(socket, buf, len); 会有较大的性能开销, 主要表现在一下两方面: 上下文切换(context switch), 此处有4次用户态和内核态的切换 Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer 其运行示意图如下 1) 先将文件内容从磁盘中拷贝到操作系统buffer 2) 再从操作系统buffer拷贝到程序应用buffer 3) 从程序buffer拷贝到socket buffer(也可以是file) 4) 从socket buffer拷贝到协议引擎. 上下文切换示意图如下 1) 调用read(), 程序切换到内核态 2) read()调用完毕, 返回数据, 程序切换回用户态 3) 调用send(), 程序切换到内核态 4) send()完毕, 程序切换回用户态 操作系统使用 read buffer 的好处是”预读”, 当你的程序需要对文件数据做处理, 并且每次读取的数据小于read buffer 的时候, 可以先将多数数据预读到 read buffer, 这样程序在读取的时候效率会更高. 但是当你需要读取的数据大于操作系统的read buffer的时候, read buffer则会成为累赘. 另外, 在你的程序不需要处理数据, 而仅仅只是做数据转移的时候, 程序buffer则会成为不必要的开销. 上面会涉及到多次上下文切换以及多次数据拷贝, 很大一部分cpu及内存开销是可以避免的, 于是有了zerocopy技术. ZeroCopyzerocopy技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 FileChannel.transferTo() 方法就是这样的实现, 这个实现是依赖于操作系统底层的sendFile()实现的. 1public void transferTo(long position, long count, WritableByteChannel target); 他的底层调用的是系统调用sendFile()方法 12#include &lt;sys/socket.h&gt;ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count); 其运行示意图如下 上下文切换示意图如下 这样, 省去了两次buffer的copy, 并且上下文切换降到了2次(调用transferTo()进入内核态, 调用完毕返回用户态) Linux 2.4 及以后的内核, 又做了改进, 不再使用socket buffer, 而是直接将read buffer数据拷贝到协议引擎, 而socket buffer只会记录数据位置的描述符和数据长度,如下 作者 [soar]2016年 05月 14日 参考献文如下[1] http://www.ibm.com/developerworks/library/j-zerocopy/]]></content>
      <tags>
        <tag>zero copy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客使用教程]]></title>
    <url>%2F2016%2F05%2F13%2Ftutorial%2F</url>
    <content type="text"><![CDATA[环境配置1.安装 node和npm，自行百度2.安装hexo-clinpm install -g hexo-cli 项目配置1.进入项目cd zhongruanhulian 2.初始化hexo hexo init 3.下载所需libnpm install ├── _config.yml //网站的 配置 信息，您可以在此配置大部分的参数。├── package.json├── scaffolds //模版 文件夹。当您新建文章时，Hexo 会根据 scaffold 来建立文件。├── source //资源文件夹是存放用户资源的地方。| ├── tags| └── _posts└── themes //主题 文件夹。Hexo 会根据主题来生成静态页面。 博客开发1.新建文章（创建一个Hello World）$ hexo new “Hello World”在/source/_post里添加hello-world.md文件，之后新建的文章都将存放在此目录下。 2.如何需要创建标签或者目录可以使用以下命令（目前已有目录和标签）hexo new page “tags”hexo new page “categories” 3.生成网站hexo generate此时会将/source的.md文件生成到/public中，形成网站的静态文件，这里的文件就是要发布到线上的文件。 4.部署本地服务器hexo server输入http://localhost:4000即可查看网站。 发布线上（这里使用ant部署线上项目，需要安装ant）ant -f upload.xml 总结每次新建好文章，编写完了，先执行hexo server命令，在本地看看效果如如何，如果没问题再执行hexo generate命令重新生成静态文件，最后执行ant -f upload.xml命令把重新生成的静态文件发布线上。 作者 [soar]2016年 05月 13日]]></content>
  </entry>
  <entry>
    <title><![CDATA[cmd markdown]]></title>
    <url>%2F2016%2F05%2F13%2Fcmd%20markdown%2F</url>
    <content type="text"><![CDATA[欢迎使用 Cmd Markdown 编辑阅读器 我们理解您需要更便捷更高效的工具记录思想，整理笔记、知识，并将其中承载的价值传播给他人，Cmd Markdown 是我们给出的答案 —— 我们为记录思想和分享知识提供更专业的工具。 您可以使用 Cmd Markdown： 整理知识，学习笔记 发布日记，杂文，所见所想 撰写发布技术文稿（代码支持） 撰写发布学术论文（LaTeX 公式支持） 除了您现在看到的这个 Cmd Markdown 在线版本，您还可以前往以下网址下载： Windows/Mac/Linux 全平台客户端 请保留此份 Cmd Markdown 的欢迎稿兼使用说明，如需撰写新稿件，点击顶部工具栏右侧的 新文稿 或者使用快捷键 Ctrl+Alt+N。 什么是 MarkdownMarkdown 是一种方便记忆、书写的纯文本标记语言，用户可以使用这些标记符号以最小的输入代价生成极富表现力的文档：譬如您正在阅读的这份文档。它使用简单的符号标记不同的标题，分割不同的段落，粗体 或者 斜体 某些文字，更棒的是，它还可以 1. 制作一份待办事宜 Todo 列表 [ ] 支持以 PDF 格式导出文稿 [ ] 改进 Cmd 渲染算法，使用局部渲染技术提高渲染效率 [x] 新增 Todo 列表功能 [x] 修复 LaTex 公式渲染问题 [x] 新增 LaTex 公式编号功能 2. 书写一个质能守恒公式[^LaTeX]$$E=mc^2$$ 3. 高亮一段代码[^code]1234567@requires_authorizationclass SomeClass: passif __name__ == '__main__': # A comment print 'hello world' 4. 高效绘制 流程图12345678st=&gt;start: Startop=&gt;operation: Your Operationcond=&gt;condition: Yes or No?e=&gt;endst-&gt;op-&gt;condcond(yes)-&gt;econd(no)-&gt;op 5. 高效绘制 序列图123Alice-&gt;Bob: Hello Bob, how are you?Note right of Bob: Bob thinksBob--&gt;Alice: I am good thanks! 6. 高效绘制 甘特图12345678910111213title 项目开发流程section 项目确定 需求分析 :a1, 2016-06-22, 3d 可行性报告 :after a1, 5d 概念验证 : 5dsection 项目实施 概要设计 :2016-07-05 , 5d 详细设计 :2016-07-08, 10d 编码 :2016-07-15, 10d 测试 :2016-07-22, 5dsection 发布验收 发布: 2d 验收: 3d 7. 绘制表格 项目 价格 数量 计算机 \$1600 5 手机 \$12 12 管线 \$1 234 8. 更详细语法说明想要查看更详细的语法说明，可以参考我们准备的 Cmd Markdown 简明语法手册，进阶用户可以参考 Cmd Markdown 高阶语法手册 了解更多高级功能。 总而言之，不同于其它 所见即所得 的编辑器：你只需使用键盘专注于书写文本内容，就可以生成印刷级的排版格式，省却在键盘和工具栏之间来回切换，调整内容和格式的麻烦。Markdown 在流畅的书写和印刷级的阅读体验之间找到了平衡。 目前它已经成为世界上最大的技术分享网站 GitHub 和 技术问答网站 StackOverFlow 的御用书写格式。 什么是 Cmd Markdown您可以使用很多工具书写 Markdown，但是 Cmd Markdown 是这个星球上我们已知的、最好的 Markdown 工具——没有之一 ：）因为深信文字的力量，所以我们和你一样，对流畅书写，分享思想和知识，以及阅读体验有极致的追求，我们把对于这些诉求的回应整合在 Cmd Markdown，并且一次，两次，三次，乃至无数次地提升这个工具的体验，最终将它演化成一个 编辑/发布/阅读 Markdown 的在线平台——您可以在任何地方，任何系统/设备上管理这里的文字。 1. 实时同步预览我们将 Cmd Markdown 的主界面一分为二，左边为编辑区，右边为预览区，在编辑区的操作会实时地渲染到预览区方便查看最终的版面效果，并且如果你在其中一个区拖动滚动条，我们有一个巧妙的算法把另一个区的滚动条同步到等价的位置，超酷！ 2. 编辑工具栏也许您还是一个 Markdown 语法的新手，在您完全熟悉它之前，我们在 编辑区 的顶部放置了一个如下图所示的工具栏，您可以使用鼠标在工具栏上调整格式，不过我们仍旧鼓励你使用键盘标记格式，提高书写的流畅度。 3. 编辑模式完全心无旁骛的方式编辑文字：点击 编辑工具栏 最右侧的拉伸按钮或者按下 Ctrl + M，将 Cmd Markdown 切换到独立的编辑模式，这是一个极度简洁的写作环境，所有可能会引起分心的元素都已经被挪除，超清爽！ 4. 实时的云端文稿为了保障数据安全，Cmd Markdown 会将您每一次击键的内容保存至云端，同时在 编辑工具栏 的最右侧提示 已保存 的字样。无需担心浏览器崩溃，机器掉电或者地震，海啸——在编辑的过程中随时关闭浏览器或者机器，下一次回到 Cmd Markdown 的时候继续写作。 5. 离线模式在网络环境不稳定的情况下记录文字一样很安全！在您写作的时候，如果电脑突然失去网络连接，Cmd Markdown 会智能切换至离线模式，将您后续键入的文字保存在本地，直到网络恢复再将他们传送至云端，即使在网络恢复前关闭浏览器或者电脑，一样没有问题，等到下次开启 Cmd Markdown 的时候，她会提醒您将离线保存的文字传送至云端。简而言之，我们尽最大的努力保障您文字的安全。 6. 管理工具栏为了便于管理您的文稿，在 预览区 的顶部放置了如下所示的 管理工具栏： 通过管理工具栏可以： 发布：将当前的文稿生成固定链接，在网络上发布，分享 新建：开始撰写一篇新的文稿 删除：删除当前的文稿 导出：将当前的文稿转化为 Markdown 文本或者 Html 格式，并导出到本地 列表：所有新增和过往的文稿都可以在这里查看、操作 模式：切换 普通/Vim/Emacs 编辑模式 7. 阅读工具栏 通过 预览区 右上角的 阅读工具栏，可以查看当前文稿的目录并增强阅读体验。 工具栏上的五个图标依次为： 目录：快速导航当前文稿的目录结构以跳转到感兴趣的段落 视图：互换左边编辑区和右边预览区的位置 主题：内置了黑白两种模式的主题，试试 黑色主题，超炫！ 阅读：心无旁骛的阅读模式提供超一流的阅读体验 全屏：简洁，简洁，再简洁，一个完全沉浸式的写作和阅读环境 8. 阅读模式在 阅读工具栏 点击 或者按下 Ctrl+Alt+M 随即进入独立的阅读模式界面，我们在版面渲染上的每一个细节：字体，字号，行间距，前背景色都倾注了大量的时间，努力提升阅读的体验和品质。 9. 标签、分类和搜索在编辑区任意行首位置输入以下格式的文字可以标签当前文档： 标签： 未分类 标签以后的文稿在【文件列表】（Ctrl+Alt+F）里会按照标签分类，用户可以同时使用键盘或者鼠标浏览查看，或者在【文件列表】的搜索文本框内搜索标题关键字过滤文稿，如下图所示： 10. 文稿发布和分享在您使用 Cmd Markdown 记录，创作，整理，阅读文稿的同时，我们不仅希望它是一个有力的工具，更希望您的思想和知识通过这个平台，连同优质的阅读体验，将他们分享给有相同志趣的人，进而鼓励更多的人来到这里记录分享他们的思想和知识，尝试点击 (Ctrl+Alt+P) 发布这份文档给好友吧！ 再一次感谢您花费时间阅读这份欢迎稿，点击 (Ctrl+Alt+N) 开始撰写新的文稿吧！祝您在这里记录、阅读、分享愉快！ 作者 @ghosert2016年 05月 13日 [^LaTeX]: 支持 LaTeX 编辑显示支持，例如：$\sum_{i=1}^n a_i=0$， 访问 MathJax 参考更多使用方法。 [^code]: 代码高亮功能支持包括 Java, Python, JavaScript 在内的，四十一种主流编程语言。]]></content>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ssd-hdd性能差异]]></title>
    <url>%2F2016%2F05%2F13%2Fssd-hdd%2F</url>
    <content type="text"><![CDATA[ssd和hdd性能差异 对于我们学软件的来说，懂点硬件知识在某种场合发挥的作用比软件还大。SSD和HDD对于大多数学计算机的都不陌生，但是要说出她们之间具体的性能差异，可能就有点难了。这里就详细介绍下她们在一些场合下的性能差异，具体场合如下： 开机 游戏 电影 音乐 总结 开机 游戏 电影 音乐 相片 总结 作者 [soar]2016年 05月 13日]]></content>
      <tags>
        <tag>hardware</tag>
      </tags>
  </entry>
</search>
